{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive LLM Agent Data Stream\n",
    "- Prompt tuning+ HumanLoop\n",
    "- WandB prompt tracing\n",
    "- Autogen Agents\n",
    "- Asyncio for async\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time  \n",
    "import autogen\n",
    "import wandb\n",
    "import humanloop\n",
    "import asyncio\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "#config_list = autogen.config_list_from_json(\"OAI_CONFIG_LIST\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS\n",
    "PROJECT_NAME = input(\"Project Name: \")\n",
    "MODEL = \"gpt-3.5-turbo-0125\"\n",
    "PROMPT_TRACING = True\n",
    "DEBUG = True  #All API Keys Exposed\n",
    "LOG_LEVEL = DEBUG\n",
    "#ENV = \"PROD\"\n",
    "ENV = \"DEV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logging (Logoru)\n",
    "if DEBUG == True:\n",
    "    logger.add(sys.stderr, format=f\"{datetime.date}\", filter=\"my_module\", level=\"DEBUG\")\n",
    "    logger.add(f\"../logs/{datetime.now()}_{PROJECT_NAME}.log\")\n",
    "else:\n",
    "    logger.add(sys.stderr, format=f\"{datetime.date}\", filter=\"my_module\", level=\"INFO\")\n",
    "    logger.add(f\"../logs/{datetime.now()}_{PROJECT_NAME}.log\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(\"./user_config.yml\").exists():\n",
    "    config_list=\"./user_config.yml\"\n",
    "    logger.info(\"userconfig file found\")\n",
    "else\n",
    "    config_list=\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG == True:\n",
    "    logger.info(f\"Weights & Biases API Key: {os.getenv('WANDB_API_KEY')}\")\n",
    "    logger.info(f\"HumanLoop API Key: {os.getenv('HUMANLOOP_API_KEY')}\")\n",
    "    logger.info(f\"OpenAI API Key: {os.getenv('OPENAI_API_KEY')}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOPs / LLMOPS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from humanloop import Humanloop\n",
    "\n",
    "\n",
    "CUSTOM_HUMANLOOP_NAME = None\n",
    "\n",
    "#Project Config\n",
    "try:\n",
    "    humanloop = Humanloop(api_key=os.getenv(\"HUMANLOOP_API_KEY\"))\n",
    "    project_response = humanloop.projects.create(name=PROJECT_NAME)\n",
    "    project_id = project_response.body[\"id\"]\n",
    "\n",
    "    #Good/Bad etc\n",
    "    #By default your project has a feedback type rating with labels good and bad.\n",
    "    '''\n",
    "    humanloop.projects.update_feedback_types(\n",
    "        id=project_id,\n",
    "        body=[{\n",
    "            \"type\": \"action\", \n",
    "            \"class\": \"multi_select\", \n",
    "            \"values\": [\n",
    "            {\"value\": \"copied\"}, {\"value\": \"saved\"}, {\"value:\": \"deleted\"}\n",
    "            ],\n",
    "        }]\n",
    "    )'''\n",
    "\n",
    "    #Register model config\n",
    "    humanloop.model_configs.register(\n",
    "        project=PROJECT_NAME,\n",
    "        model=MODEL,\n",
    "        prompt_template=\"You are a cybersecurity expert. Return detailed threat analyis for {{threat}}:\",\n",
    "        temperature=0.8,\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HumanLoop Project\n",
    "# humanloop.complete_deployed(...) will call the active model config on your project.\n",
    "# The inputs must match the input of the prompt template in your project.\n",
    "complete_response = humanloop.complete_deployed(\n",
    "    project= \"default_test\",\n",
    "    inputs={\"question\": \"How should I think about competition for my startup?\"},\n",
    "    provider_api_keys={\"openai\": os.getenv(\"OPENAI_API_KEY\"), \"anthropic\": os.getenv(\"CLAUDE_API_KEY\")},\n",
    ") \n",
    "\n",
    "# A single call to generate may return multiple outputs.\n",
    "data_id = complete_response.body[\"data\"][0][\"id\"]\n",
    "output = complete_response.body[\"data\"][0][\"output\"]\n",
    "\n",
    "# You can also access the raw response from OpenAI.\n",
    "print(complete_response.body[\"provider_responses\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_PROMPTS = True\n",
    "EXPERIMENT = False\n",
    "\n",
    "try:     \n",
    "    wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {e}\")\n",
    "    wandb.login()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project=PROJECT_NAME\n",
    "#Full list\n",
    "wandb_config = {\n",
    "     \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    \n",
    "}\n",
    "wandb.init(project=PROJECT_NAME, config=wandb_config, entity=\"orionai\")\n",
    "# For WandB Prompts\n",
    "\n",
    "#For Model Experimentation and tracking:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Metrics\n",
    " #wandb.log({\"acc\": acc, \"loss\": loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autogen Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/microsoft/autogen/blob/main/notebook/agentchat_stream.ipynb\n",
    "config_list = autogen.config_list_from_json(\"OAI_CONFIG_LIST\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sources\n",
    "- Simulating Stock Data but could do the same with CS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assistant Types\n",
    "We have designed a generic ConversableAgent class for Agents that are capable of conversing with each other through the exchange of messages to jointly finish a task. An agent can communicate with other agents and perform actions. Different agents can differ in what actions they perform after receiving messages. Two representative subclasses are AssistantAgent and UserProxyAgent\n",
    "\n",
    "The AssistantAgent is designed to act as an AI assistant, using LLMs by default but not requiring human input or code execution. It could write Python code (in a Python coding block) for a user to execute when a message (typically a description of a task that needs to be solved) is received. Under the hood, the Python code is written by LLM (e.g., GPT-4). It can also receive the execution results and suggest corrections or bug fixes. Its behavior can be altered by passing a new system message. The LLM inference configuration can be configured via [llm_config].\n",
    "\n",
    "The UserProxyAgent is conceptually a proxy agent for humans, soliciting human input as the agent's reply at each interaction turn by default and also having the capability to execute code and call functions or tools. The UserProxyAgent triggers code execution automatically when it detects an executable code block in the received message and no human user input is provided. Code execution can be disabled by setting the code_execution_config parameter to False. LLM-based response is disabled by default. It can be enabled by setting llm_config to a dict corresponding to the inference configuration. When llm_config is set as a dictionary, UserProxyAgent can generate replies using an LLM when code execution is not performed.\n",
    "\n",
    "The auto-reply capability of ConversableAgent allows for more autonomous multi-agent communication while retaining the possibility of human intervention. One can also easily extend it by registering reply functions with the register_reply() method.\n",
    "\n",
    "In the following code, we create an AssistantAgent named \"assistant\" to serve as the assistant and a UserProxyAgent named \"user_proxy\" to serve as a proxy for the human user. We will later employ these two agents to solve a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_market_news(ind, ind_upper):\n",
    "    import json\n",
    "\n",
    "    import requests\n",
    "\n",
    "    # replace the \"demo\" apikey below with your own key from https://www.alphavantage.co/support/#api-key\n",
    "    # url = 'https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers=AAPL&sort=LATEST&limit=5&apikey=demo'\n",
    "    # r = requests.get(url)\n",
    "    # data = r.json()\n",
    "    # with open('market_news_local.json', 'r') as file:\n",
    "    #     # Load JSON data from file\n",
    "    #     data = json.load(file)\n",
    "    data = {\n",
    "        \"feed\": [\n",
    "            {\n",
    "                \"title\": \"Palantir CEO Says Our Generation's Atomic Bomb Could Be AI Weapon - And Arrive Sooner Than You Think - Palantir Technologies  ( NYSE:PLTR ) \",\n",
    "                \"summary\": \"Christopher Nolan's blockbuster movie \\\"Oppenheimer\\\" has reignited the public discourse surrounding the United States' use of an atomic bomb on Japan at the end of World War II.\",\n",
    "                \"overall_sentiment_score\": 0.009687,\n",
    "            },\n",
    "            {\n",
    "                \"title\": '3 \"Hedge Fund Hotels\" Pulling into Support',\n",
    "                \"summary\": \"Institutional quality stocks have several benefits including high-liquidity, low beta, and a long runway. Strategist Andrew Rocco breaks down what investors should look for and pitches 3 ideas.\",\n",
    "                \"banner_image\": \"https://staticx-tuner.zacks.com/images/articles/main/92/87.jpg\",\n",
    "                \"overall_sentiment_score\": 0.219747,\n",
    "            },\n",
    "            {\n",
    "                \"title\": \"PDFgear, Bringing a Completely-Free PDF Text Editing Feature\",\n",
    "                \"summary\": \"LOS ANGELES, July 26, 2023 /PRNewswire/ -- PDFgear, a leading provider of PDF solutions, announced a piece of exciting news for everyone who works extensively with PDF documents.\",\n",
    "                \"overall_sentiment_score\": 0.360071,\n",
    "            },\n",
    "            {\n",
    "                \"title\": \"Researchers Pitch 'Immunizing' Images Against Deepfake Manipulation\",\n",
    "                \"summary\": \"A team at MIT says injecting tiny disruptive bits of code can cause distorted deepfake images.\",\n",
    "                \"overall_sentiment_score\": -0.026894,\n",
    "            },\n",
    "            {\n",
    "                \"title\": \"Nvidia wins again - plus two more takeaways from this week's mega-cap earnings\",\n",
    "                \"summary\": \"We made some key conclusions combing through quarterly results for Microsoft and Alphabet and listening to their conference calls with investors.\",\n",
    "                \"overall_sentiment_score\": 0.235177,\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "    feeds = data[\"feed\"][ind:ind_upper]\n",
    "    feeds_summary = \"\\n\".join(\n",
    "        [\n",
    "            f\"News summary: {f['title']}. {f['summary']} overall_sentiment_score: {f['overall_sentiment_score']}\"\n",
    "            for f in feeds\n",
    "        ]\n",
    "    )\n",
    "    return feeds_summary\n",
    "\n",
    "\n",
    "data = asyncio.Future()\n",
    "\n",
    "\n",
    "async def add_stock_price_data():\n",
    "    # simulating the data stream\n",
    "    for i in range(0, 5, 1):\n",
    "        latest_news = get_market_news(i, i + 1)\n",
    "        if data.done():\n",
    "            data.result().append(latest_news)\n",
    "        else:\n",
    "            data.set_result([latest_news])\n",
    "        # print(data.result())\n",
    "        await asyncio.sleep(5)\n",
    "\n",
    "\n",
    "data_task = asyncio.create_task(add_stock_price_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AssistantAgent (AI Agent called \"Threat Hunting\")\n",
    "threat_hunting = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config={\n",
    "        \"timeout\": 600,\n",
    "        \"cache_seed\": 41,\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0,\n",
    "    },\n",
    "    system_message=\"You are a financial expert.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a UserProxyAgent instance named \"user_proxy\" which acts as human\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=5,\n",
    "    code_execution_config=False,\n",
    "    default_auto_reply=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Redis as cache\n",
    "\n",
    "with Cache.redis(redis_url=\"redis://localhost:6379/0\") as cache:\n",
    "    user.initiate_chat(threat_hunting, message=coding_task, cache=cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def add_data_reply(recipient, messages, sender, config):\n",
    "    await asyncio.sleep(0.1)\n",
    "    data = config[\"news_stream\"]\n",
    "    if data.done():\n",
    "        result = data.result()\n",
    "        if result:\n",
    "            news_str = \"\\n\".join(result)\n",
    "            result.clear()\n",
    "            return (\n",
    "                True,\n",
    "                f\"Just got some latest market news. Merge your new suggestion with previous ones.\\n{news_str}\",\n",
    "            )\n",
    "        return False, None\n",
    "\n",
    "\n",
    "user_proxy.register_reply(autogen.AssistantAgent, add_data_reply, position=2, config={\"news_stream\": data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await user_proxy.a_initiate_chat( \n",
    "    threat_hunting,\n",
    "    message=\"\"\"Give me investment suggestion in 3 bullet points.\"\"\",\n",
    ")\n",
    "while not data_task.done() and not data_task.cancelled():\n",
    "    reply = await user_proxy.a_generate_reply(sender=threat_hunting)  # noqa: F704\n",
    "    if reply is not None:\n",
    "        await user_proxy.a_send(reply, threat_hunting)  # noqa: F704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
